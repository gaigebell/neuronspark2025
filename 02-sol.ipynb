{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6a723b",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da03c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eafcb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set_df = pd.read_csv('D:/allforwork/AI/nnsp/NS-2025-02-data/train_set.csv')\n",
    "\n",
    "origin_dataset = [[] for i in range(2)]\n",
    "siz = len(train_set_df)\n",
    "\n",
    "for i in range(len(train_set_df)):\n",
    "    text = train_set_df['review'].iloc[i]\n",
    "    label = train_set_df['label'].iloc[i]\n",
    "    origin_dataset[label].append(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44751e33",
   "metadata": {},
   "source": [
    "#### Smaller Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a401cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54073\n",
      "53916\n",
      "8638\n"
     ]
    }
   ],
   "source": [
    "n_p = 0.08\n",
    "\n",
    "small_origin_dataset = [[],[]]\n",
    "for i in range(2):\n",
    "    print(len(origin_dataset[i]))\n",
    "    n_size = math.floor(n_p * len(origin_dataset[i]))\n",
    "    small_origin_dataset[i] = random.sample(origin_dataset[i], k=n_size)\n",
    "\n",
    "origin_dataset = small_origin_dataset\n",
    "print(len(origin_dataset[0]) + len(origin_dataset[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4577aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_p = 0.15\n",
    "\n",
    "train_dataset = []\n",
    "val_dataset = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    subsiz = math.ceil(val_p * len(origin_dataset[i]))\n",
    "    val_data = random.sample(origin_dataset[i], k=subsiz)\n",
    "    for j in range(len(val_data)):\n",
    "        val_dataset.append({'text': val_data[j], 'label': i})\n",
    "\n",
    "    for j in range(len(origin_dataset[i])):\n",
    "        if origin_dataset[i][j] not in val_data:\n",
    "            train_dataset.append({'text': origin_dataset[i][j], 'label': i})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc7c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.shuffle(train_dataset)\n",
    "random.shuffle(val_dataset)\n",
    "\n",
    "train_dataset_df = pd.DataFrame(train_dataset)\n",
    "val_dataset_df = pd.DataFrame(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b75074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset_df.to_csv('D:/allforwork/AI/nnsp/NS-2025-02-data/8k_train_df.csv', index=False)\n",
    "val_dataset_df.to_csv('D:/allforwork/AI/nnsp/NS-2025-02-data/8k_val_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974588e9",
   "metadata": {},
   "source": [
    "### Laod Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "372fee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\sklearn-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"csv\", data_files='D:/allforwork/AI/nnsp/NS-2025-02-data/train_set.csv')\n",
    "dataset = dataset['train'].train_test_split(train_size=0.8, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abf3fab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 7342 examples [00:00, 332953.97 examples/s]\n",
      "Generating test split: 1296 examples [00:00, 123502.02 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7342\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1296\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={'train':'D:/allforwork/AI/nnsp/NS-2025-02-data/8k_train_df.csv',\n",
    "                                          'test':'D:/allforwork/AI/nnsp/NS-2025-02-data/8k_val_df.csv'})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d8c1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at D:/allforwork/AI/nnsp/Qwen/Qwen2_5-0_5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForSequenceClassification(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=896, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"D:/allforwork/AI/nnsp/Qwen/Qwen2_5-0_5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_mode=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.config.pad_token = tokenizer.pad_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "import torch\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a4467bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4f624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 542,464 || all params: 494,577,024 || trainable%: 0.1097\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38486df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # 添加任务指令（关键！）\n",
    "    prompt = [f\"【任务说明】\\n你是一个专业的网络内容审核助手,结合下列手册和你的专业素养,判断后面给出的网络评论是否包含辱骂、歧视、仇恨等情绪,正常评论输出0,负面评论输出1\\n【手册】\\n1.后面给出的网络评论格式为微博评论格式,形如:当前用户评论内容//@其他用户A:A的评论内容//@其他用户B:B的评论内容...\\n2.请结合其他用户评论内容作为上文来帮助你判断当前用户评论内容是否违规\\n3.评论中包含表情包，格式为[表情内容]，如：[泪]\\n4.请注意分辨反讽、玩笑、报道等容易混淆的内容\\n【待判断网络评论】: {text}\\n 答案(0/1)：\" for text in examples['text']]\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs[\"labels\"] = examples[\"label\"]\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f4000a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7342/7342 [00:02<00:00, 2800.20 examples/s]\n",
      "Map: 100%|██████████| 1296/1296 [00:00<00:00, 2608.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, batch_size=4, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98aea922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['input_ids', 'attention_mask', 'labels'], 'test': ['input_ids', 'attention_mask', 'labels']}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67f649c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"D:/allforwork/AI/nnsp/NS-02-model/v1\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"D:/allforwork/AI/nnsp/NS-02-model/logs\",  # 日志目录\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,     # 每50步记录一次\n",
    "    report_to=\"none\",     # 禁用第三方服务\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,       # 每200步验证一次\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=400,\n",
    "    # fp16=True,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    # 启用进度条增强\n",
    "    disable_tqdm=False,          # 必须开启\n",
    "    dataloader_pin_memory=True,  # 加速数据加载\n",
    "    label_names=[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f7ca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(model.config.pad_token)\n",
    "model.config.pad_token_id = tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e61ee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\47328\\AppData\\Local\\Temp\\ipykernel_10304\\2122121990.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1836' max='1836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1836/1836 10:13:25, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.669900</td>\n",
       "      <td>0.684391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.655700</td>\n",
       "      <td>0.629727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>0.507052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.293369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.160442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.128200</td>\n",
       "      <td>0.140344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>0.114439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.090166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.087964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1836, training_loss=0.3454196584250672, metrics={'train_runtime': 36825.8366, 'train_samples_per_second': 0.399, 'train_steps_per_second': 0.05, 'total_flos': 1.6169065671819264e+16, 'train_loss': 0.3454196584250672, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afceb7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c94bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'Qwen2_5-0_5B-0.08size-v1'\n",
    "trainer.save_model(f\"D:/allforwork/AI/nnsp/NS-02-model/{save_name}\")\n",
    "tokenizer.save_pretrained(f\"D:/allforwork/AI/nnsp/NS-02-model/{save_name}\")\n",
    "if isinstance(model, peft.PeftModel):\n",
    "    model.save_pretrained(f\"D:/allforwork/AI/nnsp/NS-02-model/{save_name}/lora_adapters\")\n",
    "    model.base_model.save_pretrained(f\"D:/allforwork/AI/nnsp/NS-02-model/{save_name}/base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5054beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df = pd.read_csv(\"D:/allforwork/AI/nnsp/NS-2025-02-data/test_set.csv\")\n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "for i in range(len(test_set_df)):\n",
    "    test_dataset.append(f\"【任务说明】\\n你是一个专业的网络内容审核助手,结合下列手册和你的专业素养,判断后面给出的网络评论是否包含辱骂、歧视、仇恨等情绪,正常评论输出0,负面评论输出1\\n【手册】\\n1.后面给出的网络评论格式为微博评论格式,形如:当前用户评论内容//@其他用户A:A的评论内容//@其他用户B:B的评论内容...\\n2.请结合其他用户评论内容作为上文来帮助你判断当前用户评论内容是否违规\\n3.评论中包含表情包，格式为[表情内容]，如：[泪]\\n4.请注意分辨反讽、玩笑、报道等容易混淆的内容\\n【待判断网络评论】: {test_set_df['review'].iloc[i]}\\n 答案(0/1)：\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7181514f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['【任务说明】\\n你是一个专业的网络内容审核助手,结合下列手册和你的专业素养,判断后面给出的网络评论是否包含辱骂、歧视、仇恨等情绪,正常评论输出0,负面评论输出1\\n【手册】\\n1.后面给出的网络评论格式为微博评论格式,形如:当前用户评论内容//@其他用户A:A的评论内容//@其他用户B:B的评论内容...\\n2.请结合其他用户评论内容作为上文来帮助你判断当前用户评论内容是否违规\\n3.评论中包含表情包，格式为[表情内容]，如：[泪]\\n4.请注意分辨反讽、玩笑、报道等容易混淆的内容\\n【待判断网络评论】: 回复@沂蒙山区老母鸡:如何解决保鲜问题？ //@沂蒙山区老母鸡:我想办法给您快递好吗？[太开心] //@中青旅新疆公司郑穗:我明天就试，问题是哪里去找真正的 “老母鸡”？现在的鸡，差不多都是几个月出笼。  //@北京律师成义:\\n 答案(0/1)：',\n",
       " '【任务说明】\\n你是一个专业的网络内容审核助手,结合下列手册和你的专业素养,判断后面给出的网络评论是否包含辱骂、歧视、仇恨等情绪,正常评论输出0,负面评论输出1\\n【手册】\\n1.后面给出的网络评论格式为微博评论格式,形如:当前用户评论内容//@其他用户A:A的评论内容//@其他用户B:B的评论内容...\\n2.请结合其他用户评论内容作为上文来帮助你判断当前用户评论内容是否违规\\n3.评论中包含表情包，格式为[表情内容]，如：[泪]\\n4.请注意分辨反讽、玩笑、报道等容易混淆的内容\\n【待判断网络评论】: 幻想的吧，大中午的。 //@小树林s:这牛逼吹的有点大了。瑞银的老板都脸红。[哈哈]\\n 答案(0/1)：',\n",
       " '【任务说明】\\n你是一个专业的网络内容审核助手,结合下列手册和你的专业素养,判断后面给出的网络评论是否包含辱骂、歧视、仇恨等情绪,正常评论输出0,负面评论输出1\\n【手册】\\n1.后面给出的网络评论格式为微博评论格式,形如:当前用户评论内容//@其他用户A:A的评论内容//@其他用户B:B的评论内容...\\n2.请结合其他用户评论内容作为上文来帮助你判断当前用户评论内容是否违规\\n3.评论中包含表情包，格式为[表情内容]，如：[泪]\\n4.请注意分辨反讽、玩笑、报道等容易混淆的内容\\n【待判断网络评论】: #实时旅讯# 厦门鼓浪屿昨日出现海市蜃楼！[太开心] 海上升仙山，美哭了！新年刚开始就有奇观看，今年很值得期待呢~厦门你是越来越浪漫了咩[偷笑] http://t.cn/zYGMT1w （照片源自网络）\\n 答案(0/1)：',\n",
       " '【任务说明】\\n你是一个专业的网络内容审核助手,结合下列手册和你的专业素养,判断后面给出的网络评论是否包含辱骂、歧视、仇恨等情绪,正常评论输出0,负面评论输出1\\n【手册】\\n1.后面给出的网络评论格式为微博评论格式,形如:当前用户评论内容//@其他用户A:A的评论内容//@其他用户B:B的评论内容...\\n2.请结合其他用户评论内容作为上文来帮助你判断当前用户评论内容是否违规\\n3.评论中包含表情包，格式为[表情内容]，如：[泪]\\n4.请注意分辨反讽、玩笑、报道等容易混淆的内容\\n【待判断网络评论】: 电视里在公布通缉犯的画像，姐总感觉哪里不对啊[晕]\\n 答案(0/1)：',\n",
       " '【任务说明】\\n你是一个专业的网络内容审核助手,结合下列手册和你的专业素养,判断后面给出的网络评论是否包含辱骂、歧视、仇恨等情绪,正常评论输出0,负面评论输出1\\n【手册】\\n1.后面给出的网络评论格式为微博评论格式,形如:当前用户评论内容//@其他用户A:A的评论内容//@其他用户B:B的评论内容...\\n2.请结合其他用户评论内容作为上文来帮助你判断当前用户评论内容是否违规\\n3.评论中包含表情包，格式为[表情内容]，如：[泪]\\n4.请注意分辨反讽、玩笑、报道等容易混淆的内容\\n【待判断网络评论】: 我想出去玩，好烦啊，天天都在这个楼里[怒]何日能见光明[悲伤]\\n 答案(0/1)：']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f78be22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at D:/allforwork/AI/nnsp/Qwen/Qwen2_5-0_5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"D:/allforwork/AI/nnsp/NS-02-model/v1/checkpoint-1600\"\n",
    "infer_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_mode=True)\n",
    "infer_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "infer_model.config.pad_token = infer_tokenizer.pad_token\n",
    "infer_model.config.pad_token_id = infer_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da4333c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "infer_pipeline = pipeline(\n",
    "    'text-classification',\n",
    "    model=infer_model,\n",
    "    tokenizer=infer_tokenizer,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "results = infer_pipeline(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47f7293c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a171815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9944257736206055},\n",
       " {'label': 'LABEL_1', 'score': 0.9045822024345398},\n",
       " {'label': 'LABEL_1', 'score': 0.9283919334411621},\n",
       " {'label': 'LABEL_1', 'score': 0.6515127420425415},\n",
       " {'label': 'LABEL_1', 'score': 0.9342349767684937},\n",
       " {'label': 'LABEL_1', 'score': 0.9800474047660828},\n",
       " {'label': 'LABEL_1', 'score': 0.9818713665008545},\n",
       " {'label': 'LABEL_1', 'score': 0.8558772802352905},\n",
       " {'label': 'LABEL_0', 'score': 0.6429874300956726},\n",
       " {'label': 'LABEL_1', 'score': 0.7952083945274353},\n",
       " {'label': 'LABEL_1', 'score': 0.7754880785942078},\n",
       " {'label': 'LABEL_1', 'score': 0.9683642387390137},\n",
       " {'label': 'LABEL_1', 'score': 0.6028019785881042},\n",
       " {'label': 'LABEL_1', 'score': 0.8764522075653076},\n",
       " {'label': 'LABEL_1', 'score': 0.9472465515136719}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1b4ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = [1 if res['label'] == 'LABEL_1' else 0 for res in results ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65d352ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_result[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efa097a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "ans = {\"results\": []}\n",
    "for t, res in enumerate(output_result):\n",
    "    ans[\"results\"].append({\"id\":t + 1,\"label\":res})\n",
    "\n",
    "with open(\"D:/allforwork/AI/nnsp/NS-2025-02-data/results.json\", mode=\"w\") as f:\n",
    "    json.dump(ans, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c3517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
